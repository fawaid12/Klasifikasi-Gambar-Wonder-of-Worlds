# -*- coding: utf-8 -*-
"""Klasifikasi Gambar_Muhammad Fawaid As'ad.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OdfajBSg6uu0Ifx50gD7RHw95ss4O4dI

# Proyek Klasifikasi Gambar: Wonders of World
- **Nama:** Muhammad Fawaid As'ad
- **Email:** muhammadfawaidasad@gmail.com
- **ID Dicoding:** fawaid27

## Import Semua Packages/Library yang Digunakan
"""

import os
import shutil
import splitfolders
from collections import Counter
import kagglehub

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNet
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D, Input, LeakyReLU
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix

import random
import cv2

"""## Data Preparation

### Data Loading
"""

# Download dataset dari KaggleHub
dataset_path = kagglehub.dataset_download("balabaskar/wonders-of-the-world-image-classification")
raw_data_dir = os.path.join(dataset_path, "wonders_dataset")

# Hitung kelas dengan gambar terbanyak
def count_top_classes(directory, top_n=4):
    class_counts = {}
    for class_name in os.listdir(directory):
        class_path = os.path.join(directory, class_name)
        if os.path.isdir(class_path):
            count = len([
                file for file in os.listdir(class_path)
                if file.lower().endswith(('jpg', 'jpeg', 'png'))
            ])
            class_counts[class_name] = count
    sorted_counts = Counter(class_counts).most_common(top_n)
    print(f"\n{top_n} kelas dengan jumlah gambar terbanyak:")
    for class_name, count in sorted_counts:
        print(f"{class_name}: {count} gambar")
    return [cls for cls, _ in sorted_counts]

top_classes = count_top_classes('Wonders of World')

# Salin 4 kelas teratas ke folder baru
filtered_dir = "wonders_dataset_filtered"
if os.path.exists(filtered_dir):
    shutil.rmtree(filtered_dir)
os.makedirs(filtered_dir)

for cls in top_classes:
    src = os.path.join('Wonders of World', cls)
    dst = os.path.join(filtered_dir, cls)
    shutil.copytree(src, dst)

"""### Data Preprocessing

#### Split Dataset
"""

# Split ke train/val/test
splitfolders.ratio(filtered_dir, output="wonders_split", seed=42, ratio=(.7, .15, .15))

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    zoom_range=0.2,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    fill_mode='nearest'
)
val_test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    "wonders_split/train",
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)
val_generator = val_test_datagen.flow_from_directory(
    "wonders_split/val",
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)
test_generator = val_test_datagen.flow_from_directory(
    "wonders_split/test",
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

num_classes = train_generator.num_classes

"""## Modelling"""

# Load MobileNet tanpa top (tidak termasuk classifier-nya)
mobilenet_base = MobileNet(include_top=False, weights='imagenet', input_shape=(224, 224, 3))

mobilenet_base.trainable = True
for layer in mobilenet_base.layers[:-20]:
    layer.trainable = False

# Buat model Sequential
model = Sequential([
    Input(shape=(224, 224, 3)),

    # Tambahkan base MobileNet
    mobilenet_base,

    # Tambah custom conv layers (optional)
    Conv2D(64, (3, 3), padding='same'),
    LeakyReLU(negative_slope=0.1),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    Conv2D(128, (3, 3), padding='same'),
    LeakyReLU(negative_slope=0.1),
    BatchNormalization(),
    MaxPooling2D(pool_size=(2, 2)),

    GlobalAveragePooling2D(),
    Dense(128),
    LeakyReLU(negative_slope=0.1),
    Dropout(0.4),
    Dense(num_classes, activation='softmax')
])

# Kompilasi model
model.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

callbacks = [
    EarlyStopping(patience=5, restore_best_weights=True),
    ModelCheckpoint("best_model_mobilenet.keras", save_best_only=True),
    ReduceLROnPlateau(factor=0.5, patience=3, min_lr=1e-6, verbose=1),
]

history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=10,
    callbacks=callbacks
)

"""## Evaluasi dan Visualisasi"""

# Evaluasi Train & Test
train_loss, train_acc = model.evaluate(train_generator)
test_loss, test_acc = model.evaluate(test_generator)

print(f"\nðŸ“Š Train Accuracy: {train_acc*100:.2f}%")
print(f"ðŸ“Š Test Accuracy: {test_acc*100:.2f}%")

# Plot akurasi & loss
def plot_training_history(history):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs = range(1, len(acc) + 1)

    plt.figure()
    plt.plot(epochs, acc, 'b', label='Training acc')
    plt.plot(epochs, val_acc, 'g', label='Validation acc')
    plt.title('Training and validation accuracy')
    plt.legend()
    plt.show()

    plt.figure()
    plt.plot(epochs, loss, 'b', label='Training loss')
    plt.plot(epochs, val_loss, 'g', label='Validation loss')
    plt.title('Training and validation loss')
    plt.legend()
    plt.show()

plot_training_history(history)

# Confusion matrix
def plot_confusion_matrix(model, test_generator):
    Y_pred = model.predict(test_generator)
    y_pred = np.argmax(Y_pred, axis=1)
    cm = confusion_matrix(test_generator.classes, y_pred)
    class_names = list(test_generator.class_indices.keys())

    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap="Blues")
    plt.title("Confusion Matrix")
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()

    print("\nClassification Report:\n")
    print(classification_report(test_generator.classes, y_pred, target_names=class_names))

plot_confusion_matrix(model, test_generator)

"""Analisis
Berdasarkan hasil dari data tersebut dihasilkan :

ðŸ”¹ Akurasi train vs test hampir sama tinggi (98.91% vs 99.17%)

ðŸ”¹ Loss train vs test juga hampir sama (0.0288 vs 0.0304)

ðŸ”¹ Classification report memperlihatkan skor f1, precision, dan recall yang konsisten di semua kelas

## Konversi Model
"""

model.export('submission/saved_model')

# Konversi ke TFLite
converter = tf.lite.TFLiteConverter.from_saved_model("submission/saved_model")
tflite_model = converter.convert()
os.makedirs("submission/tflite", exist_ok=True)
with open("submission/tflite/model.tflite", "wb") as f:
    f.write(tflite_model)

# Save labels
class_names = list(train_generator.class_indices.keys())
with open("submission/tflite/label.txt", "w") as f:
    for item in class_names:
        f.write("%s\n" % item)

!tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model submission/saved_model submission/tfjs_model

# Simpan file requirements
with open("submission/requirements.txt", "w") as f:
    f.write("\n".join([
        "tensorflow",
        "split-folders",
        "tensorflowjs"
    ]))

"""## Inference (Optional)"""